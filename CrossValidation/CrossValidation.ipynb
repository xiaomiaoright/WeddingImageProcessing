{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Introduction to Cross Validation and Model Selection\n\nIn a previous lab you created a model with l2 or ridge regularization and l1 or lasso regularization. In both cases, an apparently optimum value of the regularization parameter was found. This process is an example of **model selection**. The goal of model selection is to find the best performing model for the problem at hand. Model selection is a very general term and can apply to at least the following common cases:\n- Selection of optimal model **hyperparameters**. Hyperparameters are parameters which determine the characteristics of a model. Hyperparameters are distinct from the model parameters. For example, for the case of l2 regularized regression, the degree of regularization is determined by a hyperparameter, which is distinct from the regression coefficients or parameters. \n- **Feature selection** is the process of determining which features should be used in a model. \n- Comparing different model types is an obvious case of model selection. \n\nIf you are thinking that the model selection process is closely related to model training, you are correct. Model selection is a component of model training. However, one must be careful, as applying a poor model selection method can lead to an over-fit model!\n\n## Overview of k-fold cross validation\n\nThe questions remain, how good are the hyperparameter estimates previously obtained for the l2 and l1 regularization parameters and are there better ways to estimate these parameters? The answer to both questions is to use **resampling methods**. Resampling methods repeat a calculation multiple times using randomly selected subsets of the complete dataset.  In fact, resampling methods are generally the best approach to model selection problems. \n\n\n**K-folod Cross validation** is a widely used resampling method. In cross validation a dataset is divided into **k folds**. Each fold contains $\\frac{1}{k}$ cases and is created by **Bernoulli random sampling** of the full data set. A computation is performed on $k-1$ folds of the full dataset. The $k^{th}$ fold is **held back** and is used for testing the result. The computation is performed $k$ times and model parameters are averaged (mean taken) over the results of the $k$ folds. For each iteration, $k-1$ folds are used for training and the $k^{th}$ fold is used for testing. \n\n4-fold cross validation is illustrated in the figure below. To ensure the data are randomly sampled the data is randomly shuffled at the start of the procedure. The random samples can then be efficiently sub-sampled as shown in the figure. The model is trained and tested four times. For each iteration the data is trained with three folds of the data and tested with the fold shown in the dark shading. \n\n<img src=\"CrossValidation.jpg\" alt=\"Drawing\" style=\"width:750px; height:400px\"/>\n<center> **Resampling scheme for 4-fold cross validation**</center>\n\n## Introduction to nested cross validation\n\nUnfortunately, simple cross validation alone does not provide an unbiased approach to model selection. The problem with evaluating model performance with simple cross validation uses the same data samples as the model selection process. This situation will lead to model over fitting wherein the model selection is learned based on the evaluation data. The result is usually unrealistically optimistic model performance estimates.\n\nTo obtain unbiased estimates of expected model performance while performing model selection, it is necessary to use **nested cross validation**. As the name implies, nested cross validation is performed though a pair of nested CV loops. The outer loop uses a set of folds to perform model evaluation. The inner loop performs model selection using another randomly sampled set of  folds not used for evaluation by the outer loop. This algorithm allows model selection and evaluation to proceed with randomly sampled subsets of the full data set, thereby avoiding model selection bias. \n\n## Cross validation and computational efficiency\n\nAs you may have surmised, cross validation can be computationally intensive. Processing each fold of a cross validation requires fitting and evaluating the model. It is desirable to compute a reasonable number of folds. Since the results are averaged over the folds, a small number of folds can lead to significant variability in the final result. However, with large data sets or complex models, the number of folds must be limited in order to complete the cross validation process in a reasonable amount of time. It is, therefore, necessary to trade off accuracy of the cross validation result with the practical consideration of the required computational resources. \n\nAs mentioned earlier, other resampling methods exist. For example, leave-one-out resampling has the same number of folds as data cases. Such methods provide optimal unbiased estimates of model performance. Unfortunately, as you might think, such methods are computationally intensive and are only suitable for small datasets. In practice k-fold cross validation is a reasonable way to explore bias-variance trade-off with reasonable computational resources. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Load Features and Labels\n\nWith the above theory in mind, you will now try an example. \n\nAs a first step, execute the code in the cell below to load the packages required for this notebook. "
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport sklearn.model_selection as ms\nfrom sklearn import linear_model\nimport sklearn.metrics as sklm\nfrom sklearn import cross_validation\nimport numpy as np\nimport numpy.random as nr\nimport matplotlib.pyplot as plt\nimport math\n\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, load the preprocessed files containing the features and the labels. The preprocessing includes the following:\n1. Cleaning missing values.\n2. Aggregate categories of certain categorical variables. \n3. Encoding categorical variables as binary dummy variables.\n4. Standardization of numeric variables. \n\nExecute the code in the cell below to load the features and labels as numpy arrays for the example. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Features = np.array(pd.read_csv('Credit_Features.csv'))\nLabels = np.array(pd.read_csv('Credit_Labels.csv'))\nprint(Features.shape)\nprint(Labels.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Construct the logistic regression model\n\nTo create a baseline for comparison you will now create a logistic regression model without cross validation. This model uses a fixed set of hyperparameters. You will compare the performance of this model with the cross validation results computed subsequently. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, execute the code in the cell below to create training and testing splits of the dataset. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Randomly sample cases to create independent training and test data\nnr.seed(1115)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 300)\nX_train = Features[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nX_test = Features[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, it is time to compute the logistic regression model. The code in the cell below does the following:\n1. Define a logistic regression model object using the `LogisticRegression` method from the scikit-learn `linear_model` package.\n2. Fit the linear model using the numpy arrays of the features and the labels for the training data set.\n\nExecute this code. "
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "logistic_mod = linear_model.LogisticRegression(C = 1.0, class_weight = {0:0.45, 1:0.55}) \nlogistic_mod.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The code in the cell below computes and displays metrics and the ROC curve for the model using the test data subset. Execute this code and examine the results. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n\ndef print_metrics(labels, probs, threshold):\n    scores = score_model(probs, threshold)\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n\ndef plot_auc(labels, probs):\n    ## Compute the false positive rate, true positive rate\n    ## and threshold along with the AUC\n    fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1])\n    auc = sklm.auc(fpr, tpr)\n    \n    ## Plot the result\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()    \n\nprobabilities = logistic_mod.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.3)  \nplot_auc(y_test, probabilities)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "These results look promising, with most of the metrics having reasonable values. The question is now, how will these performance estimates hold up to cross validation?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Cross validate model\n\nTo compute a better estimate of model performance, you can perform simple cross validation. The code in the cell performs the following processing:\n1. Create a list of the metrics to be computed for each fold. \n2. Defines a logistic regression model object.\n3. A 10 fold cross validation is performed using the `cross_validate` function from the scikit-learn `model_selection` package.\n\nExecute this code. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Labels = Labels.reshape(Labels.shape[0],)\nscoring = ['precision_macro', 'recall_macro', 'roc_auc']\nlogistic_mod = linear_model.LogisticRegression(C = 1.0, class_weight = {0:0.45, 1:0.55}) \nscores = ms.cross_validate(logistic_mod, Features, Labels, scoring=scoring,\n                        cv=10, return_train_score=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The code in the cell below displays the performance metrics along with the mean and standard deviation, computed for each fold to the cross validation. The 'macro' versions of precision and recall are used. These macro versions average over the positive and negative cases. \n\nExecute this code, examine the result, and answer **Question 1** on the course page."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def print_format(f,x,y,z):\n    print('Fold %2d    %4.3f        %4.3f      %4.3f' % (f, x, y, z))\n\ndef print_cv(scores):\n    fold = [x + 1 for x in range(len(scores['test_precision_macro']))]\n    print('         Precision     Recall       AUC')\n    [print_format(f,x,y,z) for f,x,y,z in zip(fold, scores['test_precision_macro'], \n                                          scores['test_recall_macro'],\n                                          scores['test_roc_auc'])]\n    print('-' * 40)\n    print('Mean       %4.3f        %4.3f      %4.3f' % \n          (np.mean(scores['test_precision_macro']), np.mean(scores['test_recall_macro']), \n           np.mean(scores['test_roc_auc'])))  \n    print('Std        %4.3f        %4.3f      %4.3f' % \n          (np.std(scores['test_precision_macro']), np.std(scores['test_recall_macro']), \n           np.std(scores['test_roc_auc'])))\n\nprint_cv(scores)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that there is considerable variability in each of the performance metrics from fold to fold. Even so, the standard deviations are at least an order of magnitude than the means. It is clear that **any one fold does not provide a representative value of the performance metrics**. The later is a key point as to why cross validation is important when evaluating a machine learning model.  \n\nCompare the performance metric values to the values obtained for the baseline model you created above. In general the metrics obtained by cross validation are lower. However, the metrics obtained for the baseline model are within 1 standard deviation of the average metrics from cross validation. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Optimize hyperparameters with nested cross validation\n\nGiven the variability observed in cross validation, it should be clear that performing model selection from a single training and evaluation is an uncertain proposition at best. Fortunately, the nested cross validation approach provides a better way to perform model selection. However, there is no guarantee that a model selection process will, in fact, improve a model. In some cases, it may prove to be that model selection has minimal impact. \n\nTo start the nested cross validation process it is necessary to define the randomly sampled folds for the inner and outer loops. The code in the cell below uses the `KFolds` function from the scikit-learn `model_selection` package to define fold selection objects. Notice that the `shuffle = True` argument is used in both cases. This argument specifies that a random shuffle is preformed before folds are created, ensuring that the sampling of the folds for the inside and outside loops are independent. Notice that by creating these independent fold objects there is no need to actually create nested loops for this process. \n\nExecute this code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(123)\ninside = ms.KFold(n_splits=10, shuffle = True)\nnr.seed(321)\noutside = ms.KFold(n_splits=10, shuffle = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "An important decision in model selection searches is the choice of performance metric used to find the best model. For classification problems scikit-learn uses accuracy as the default metric. However, as you have seen previously, accuracy is not necessarily the best metric, particularly when there is a class imbalance as is the case here. There are a number of alternatives which one could choose for such a situation. In this case AUC will be used. \n\nThe code below uses the `inside` k-fold object to execute the inside loop of the nested cross validation. Specifically, the steps are:\n1. Define a dictionary with the grid of parameter values to search over. In this case there is only one parameter, `C`, with a list of values to try. In a more general case, the dictionary can contain values from multiple parameters, creating a multi-dimensional grid that the cross validation process will iterate over. In this case there are 5 hyperparameter values in the grid and 10-fold cross validation is being used. Thus, the model will be trained and evaluated 50 times. \n2. The logistic regression model object is defined. \n3. The cross validation search over the parameter grid is performed using the `GridSearch` function from the scikit-learn `model_selection` package. Notice that the cross validation folds are computed using the `inside` k-fold object.\n\n\n****\n**Note:** Somewhat confusingly, the scikit-learn `LogisticRegression` function uses a regularization parameter `C` which is the inverse of the usual l2 regularization parameter $\\lambda$. Thus, the smaller the parameter the stronger the regulation \n****\n\nExecute this code."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(3456)\n## Define the dictionary for the grid search and the model object to search on\nparam_grid = {\"C\": [0.1, 1, 10, 100, 1000]}\n## Define the logistic regression model\nlogistic_mod = linear_model.LogisticRegression(class_weight = {0:0.45, 0:0.55}) \n\n## Perform the grid search over the parameters\nclf = ms.GridSearchCV(estimator = logistic_mod, param_grid = param_grid, \n                      cv = inside, # Use the inside folds\n                      scoring = 'roc_auc',\n                      return_train_score = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The cross validated grid search object, `clf`, has been created. \n\nThe code in the cell below fits the cross validated model using the `fit`method. The AUC for each hyperparameter and fold is displayed as an array. Finally, the hyperparameter for the model with the best average AUC is displayed.  Execute this code and  examine the results."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Fit thhe cross validated grid search over the data \nclf.fit(Features, Labels)\nkeys = list(clf.cv_results_.keys())\nfor key in keys[6:16]:\n    print(clf.cv_results_[key])\n## And print the best parameter value\nclf.best_estimator_.C",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The array of AUC metrics has dimensions 10 folds X  hyperparameter values. As you might expect by now, there is considerable variation in the AUC from fold to fold for each hyperparamter value, or column. \n\nEvidently, the optimal hyperparameter value is 0.1. \n\nTo help understand this behavior a bit more, the code in the cell below does the following:\n1. Compute and display the mean and standard deviation of the AUC for each hyperparameter value.\n2. Plot the AUC values for each fold vs. the hyperparameter values. The mean AUC for each hyperparameter value is shown with a red +. \n\nExecute this code and examine the results. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plot_cv(clf, params_grid, param = 'C'):\n    params = [x for x in params_grid[param]]\n  \n    keys = list(clf.cv_results_.keys())              \n    grid = np.array([clf.cv_results_[key] for key in keys[6:16]])\n    means = np.mean(grid, axis = 0)\n    stds = np.std(grid, axis = 0)\n    print('Performance metrics by parameter')\n    print('Parameter   Mean performance   STD performance')\n    for x,y,z in zip(params, means, stds):\n        print('%8.2f        %6.5f            %6.5f' % (x,y,z))\n    \n    params = [math.log10(x) for x in params]\n    \n    plt.scatter(params * grid.shape[0], grid.flatten())\n    p = plt.scatter(params, means, color = 'red', marker = '+', s = 300)\n    plt.plot(params, np.transpose(grid))\n    plt.title('Performance metric vs. log parameter value\\n from cross validation')\n    plt.xlabel('Log hyperparameter value')\n    plt.ylabel('Performance metric')\n    \nplot_cv(clf, param_grid)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are a number of points to notice here:\n1. The mean AUC for each value of the hyperparameter are all within 1 standard deviation of each other. This result indicates that model performance is not sensitive to the choice of hyperparamter. \n2. Graphically you can see that there is a noticeable variation in the AUC from metric to metric, regardless of hyperparameter. Keep in mind that **this variation is simply a result of random sampling of the data!**\n\nFinally, it is time to try execute the outer loop of the nested cross validation to evaluate the performance of the 'best' model selected by the inner loop. In this case, 'best' is quite approximate, since as already noted, the differences in performance between the models is not significant. \n\nThe code in the cell below executes the outer loop of the nested cross validation using the `cross_val_scores` function from the scikit-learn `model_selection` package. The folds are determined by the `outside` k-fold object. The mean and standard deviation of the AUC is printed along with the value estimated for each fold. Execute this code and examine the result. \n\nThen, answer **Question 2** on the course page."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(498)\ncv_estimate = ms.cross_val_score(clf, Features, Labels, \n                                 cv = outside) # Use the outside folds\nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As expected, there is considerable variation in AUC across the folds. The mean AUC is a bit lower than estimated for the inner loop of the nested cross validation and the baseline model. However, all of these values are within 1 standard deviation of each other, and thus these differences cannot be considered significant. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, you will build and test a model using the estimated optimal hyperparameters. Then, answer **Question 3** on the course page. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "logistic_mod = linear_model.LogisticRegression(C = 0.1, class_weight = {0:0.45, 1:0.55}) \nlogistic_mod.fit(X_train, y_train)\nprobabilities = logistic_mod.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.3)  \nplot_auc(y_test, probabilities)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Summary\n\nIn this lab you have performed by simple cross validation and nested cross validation. Key points and observations are:\n1. Model selection should be done using a resampling procedure such as nested cross validation. The nested sampling structure is required to prevent bias in model selection wherein the model selected learns the best hyperparameters for the samples used, rather than a model that generalizes well. \n2. There is significant variation in model performance from fold to fold in cross validation. This variation arises from the sampling of the data alone and is not a property of any particular model.\n3. Given the expected sampling variation in cross validation, there is generally considerable uncertainty as to which model is best when performing model selection.  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}